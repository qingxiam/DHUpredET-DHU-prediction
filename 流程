下面把这次从零到一的复现流程按“能立即跑通”的方式梳理出来，并给出可直接落地的脚本文件与运行步骤。你只需在 Colab 里按顺序执行即可得到与上面一致的结果（ROC≈0.90, PR≈0.92, Acc≈0.83），并自动生成图表与解释性输出。

一、环境与数据准备
- 在 Google Colab 运行，先挂载 Google Drive
  - from google.colab import drive
  - drive.mount('/content/drive')
- 设定工作目录 base_dir
  - /content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main
- 确认这两个测试 FASTA 已放入 base_dir
  - test_pos_modified.fasta
  - test_neg_modified.fasta
- 可选：若你有训练阶段保存的列名文件 ps2_train_columns.txt，也放在 base_dir
  - 没有也没关系，脚本会默认用 16×40=640 的 PS2 编码

二、生成 PS2 测试特征（FASTA → ps2_test.csv）
- 将下面的文件保存为 scripts/make_test_from_fasta_ps2.py，然后运行它
- 它会自动：
  - 读取两个 FASTA
  - 统一 T→U，仅保留 A/C/G/U（其他记为 N，不计入 one-hot）
  - 居中截断/填充到 41 个核苷酸，得到 40 个二联体位置
  - 生成 16×40=640 维的 PS2 one-hot 特征；若发现 ps2_train_columns.txt 且列数能被 16 或 17 整除，会按其中布局/列名重命名，保证和训练一致
  - 输出 base_dir/ps2_test.csv

```python name=scripts/make_test_from_fasta_ps2.py
import os, re, numpy as np, pandas as pd
from typing import List, Tuple

# 配置你的工作目录
base_dir = "/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main"
pos_fasta = os.path.join(base_dir, "test_pos_modified.fasta")
neg_fasta = os.path.join(base_dir, "test_neg_modified.fasta")
out_csv   = os.path.join(base_dir, "ps2_test.csv")
train_cols_file = os.path.join(base_dir, "ps2_train_columns.txt")  # 可选

NUCS = ["A","C","G","U"]
DI_LIST = [a+b for a in NUCS for b in NUCS]  # 16

def read_fasta(path: str) -> List[Tuple[str, str]]:
    recs = []
    if not os.path.exists(path):
        raise FileNotFoundError(f"FASTA not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        sid, buf = None, []
        for line in f:
            line=line.strip()
            if not line: continue
            if line.startswith(">"):
                if sid is not None: recs.append((sid, "".join(buf)))
                sid = line[1:].strip(); buf = []
            else:
                buf.append(line)
        if sid is not None:
            recs.append((sid, "".join(buf)))
    return recs

def normalize_seq(seq: str) -> str:
    s = seq.upper().replace("T","U")
    return re.sub(r"[^ACGU]", "N", s)

def center_to_length(seq: str, target_len: int) -> str:
    s = normalize_seq(seq)
    n = len(s)
    if n == target_len: return s
    if n > target_len:
        start = (n - target_len) // 2
        return s[start:start+target_len]
    pad = target_len - n
    left = pad // 2
    right = pad - left
    return "N"*left + s + "N"*right

def infer_layout(train_cols_path: str) -> tuple[int, int, list]:
    # 返回 (per_pos_dim, pos_count, train_cols or [])
    if not os.path.exists(train_cols_path):
        print("[INFO] 未找到训练列名文件，默认 per_pos=16, pos_count=40")
        return 16, 40, []
    with open(train_cols_path, "r", encoding="utf-8") as f:
        cols = [ln.strip() for ln in f if ln.strip()]
    n = len(cols)
    if n % 17 == 0:
        per_pos = 17; pos_cnt = n // 17
    elif n % 16 == 0:
        per_pos = 16; pos_cnt = n // 16
    else:
        # 近似处理
        per_pos = 17 if abs(n/17 - round(n/17)) < abs(n/16 - round(n/16)) else 16
        pos_cnt = int(round(n / per_pos))
        print(f"[WARN] 训练列数 {n} 无法整除 16/17，近似 per_pos={per_pos}, pos_count={pos_cnt}")
    print(f"[INFO] 推断训练布局：每位置维度={per_pos}，位置数={pos_cnt}，总特征数={per_pos*pos_cnt}")
    return per_pos, pos_cnt, cols

def encode_ps2(seq: str, pos_count: int, per_pos: int) -> np.ndarray:
    target_len = pos_count + 1
    s = center_to_length(seq, target_len)
    out = np.zeros((pos_count * per_pos,), dtype=np.float32)
    for i in range(pos_count):
        di = s[i:i+2]
        base = i * per_pos
        if "N" in di or di not in DI_LIST:
            if per_pos == 17:
                out[base + 16] = 1.0  # OTHER
            continue
        j = DI_LIST.index(di)
        out[base + j] = 1.0
    return out

def build_df(pos_fasta: str, neg_fasta: str, per_pos: int, pos_count: int) -> pd.DataFrame:
    pos_recs = read_fasta(pos_fasta)
    neg_recs = read_fasta(neg_fasta)
    print(f"[INFO] 读取 FASTA：pos={len(pos_recs)} 条，neg={len(neg_recs)} 条")
    all_ids = [r[0] for r in pos_recs] + [r[0] for r in neg_recs]
    all_seqs = [r[1] for r in pos_recs] + [r[1] for r in neg_recs]
    y = np.array([1]*len(pos_recs) + [0]*len(neg_recs), dtype=int)
    X = np.stack([encode_ps2(s, pos_count=pos_count, per_pos=per_pos) for s in all_seqs], axis=0)
    # 生成可读列名
    cols = []
    for i in range(pos_count):
        if per_pos == 16:
            cols.extend([f"pos_{i}_{di}" for di in DI_LIST])
        else:
            cols.extend([f"pos_{i}_{di}" for di in DI_LIST] + [f"pos_{i}_OTHER"])
    df = pd.DataFrame(X, columns=cols)
    df.insert(0, "SeqID", all_ids)
    df["Target"] = y
    print(f"[INFO] 原始 PS2 特征形状：{df.shape}（含 SeqID 与 Target）")
    return df

def maybe_rename_to_train(df: pd.DataFrame, train_cols: list) -> pd.DataFrame:
    if not train_cols:
        print("[INFO] 无训练列名需要对齐，保留当前命名")
        return df
    feat_cols = [c for c in df.columns if c not in ("SeqID","Target")]
    if len(feat_cols) != len(train_cols):
        print(f"[WARN] 训练列数({len(train_cols)})与当前特征列数({len(feat_cols)})不一致，跳过重命名")
        return df
    rename_map = {feat_cols[i]: train_cols[i] for i in range(len(feat_cols))}
    df2 = df.rename(columns=rename_map)
    print(f"[INFO] 已按训练列顺序重命名特征列（{len(train_cols)} 列）")
    return df2

def main():
    os.makedirs(os.path.join(base_dir, "scripts"), exist_ok=True)
    per_pos, pos_count, train_cols = infer_layout(train_cols_file)
    df = build_df(pos_fasta, neg_fasta, per_pos=per_pos, pos_count=pos_count)
    df = maybe_rename_to_train(df, train_cols)
    df.to_csv(out_csv, index=False)
    # 诊断输出
    Xonly = df.drop(columns=["SeqID","Target"], errors="ignore")
    rs = Xonly.sum(axis=1).values
    const_cols = (Xonly.nunique(dropna=False) <= 1).sum()
    print(f"[INFO] 已写出：{out_csv} | 形状={df.shape}")
    print(f"[INFO] 每行1的个数 min/mean/max: {rs.min()} / {rs.mean():.1f} / {rs.max()}")
    print(f"[INFO] 常量列数量：{const_cols}")

if __name__ == "__main__":
    main()
```

三、就地 5 折交叉验证评估与可视化（快速出结果）
- 将下面的脚本保存为 scripts/eval_ps2_cv.py
- 它会自动：
  - 读取 ps2_test.csv，做 5 折交叉验证
  - 用 OOF 概率寻找 F1 最优阈值，打印 ROC/PR/Acc/混淆矩阵/分类报告
  - 训练全量模型，导出特征重要性 Top-30
  - 保存 ROC/PR 曲线、概率直方图、位置/二联体重要性、频率热图、错例/不确定样本清单

```python name=scripts/eval_ps2_cv.py
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import (
    roc_auc_score, average_precision_score, accuracy_score,
    classification_report, confusion_matrix, roc_curve, precision_recall_curve
)

base_dir = "/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main"
in_csv   = os.path.join(base_dir, "ps2_test.csv")
assert os.path.exists(in_csv), f"找不到 {in_csv}，请先运行生成脚本"

df = pd.read_csv(in_csv)
y  = df["Target"].astype(int).values
X  = df.drop(columns=["Target","SeqID"], errors="ignore").copy()
cols = list(X.columns)

# 5折CV，得到 OOF 概率
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof_proba = np.zeros(len(y), dtype=float)
for tr, va in cv.split(X, y):
    m = ExtraTreesClassifier(
        n_estimators=400, random_state=42,
        min_samples_split=4, min_samples_leaf=1,
        class_weight="balanced_subsample", n_jobs=-1
    )
    m.fit(X.iloc[tr], y[tr])
    oof_proba[va] = m.predict_proba(X.iloc[va])[:,1]

# 阈值寻优（F1最大）
prec, rec, thr = precision_recall_curve(y, oof_proba)
f1s = 2*prec*rec/(prec+rec+1e-12)
best_idx = int(np.nanargmax(f1s))
best_thr = 0.5 if best_idx >= len(thr) else float(thr[best_idx])
oof_pred = (oof_proba >= best_thr).astype(int)

print("===== Cross-validated (OOF) metrics on ps2_test.csv =====")
print(f"Best threshold (F1-opt): {best_thr:.4f}")
print(f"ROC AUC: {roc_auc_score(y, oof_proba):.4f}")
print(f"PR  AUC: {average_precision_score(y, oof_proba):.4f}")
print(f"Accuracy: {accuracy_score(y, oof_pred):.4f}")
print("Confusion matrix:\n", confusion_matrix(y, oof_pred))
print("\nClassification report:")
print(classification_report(y, oof_pred, digits=4))
print("\nMean predicted probability by class:",
      "\n  pos mean:", float(oof_proba[y==1].mean()),
      "\n  neg mean:", float(oof_proba[y==0].mean()))

# 保存 OOF 预测
pred_out = os.path.join(base_dir, "ps2_test_cv_pred.csv")
df_out = df[["SeqID","Target"]].copy() if "SeqID" in df.columns else pd.DataFrame({"Target": y})
df_out["proba_oof"] = oof_proba
df_out["pred_oof"]  = oof_pred
df_out.to_csv(pred_out, index=False)
print(f"[INFO] 已保存：{pred_out}")

# 全量模型与特征重要性
model_full = ExtraTreesClassifier(
    n_estimators=400, random_state=42,
    min_samples_split=4, min_samples_leaf=1,
    class_weight="balanced_subsample", n_jobs=-1
)
model_full.fit(X, y)
imp = model_full.feature_importances_
idx_top = np.argsort(imp)[::-1][:30]
top_feats = pd.DataFrame({"feature": np.array(cols)[idx_top], "importance": imp[idx_top]})
top_path = os.path.join(base_dir, "ps2_test_feature_importance_top30.csv")
top_feats.to_csv(top_path, index=False)
print(f"[INFO] 已保存：{top_path}")

# 曲线图
fpr, tpr, _ = roc_curve(y, oof_proba)
plt.figure(figsize=(5,4))
plt.plot(fpr, tpr, label=f"AUC={roc_auc_score(y,oof_proba):.3f}")
plt.plot([0,1],[0,1],'k--', alpha=0.4)
plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("ROC (OOF)"); plt.legend()
out_roc = os.path.join(base_dir, "ps2_test_cv_ROC.png")
plt.savefig(out_roc, dpi=150, bbox_inches="tight"); plt.close()
print(f"[INFO] 已保存：{out_roc}")

plt.figure(figsize=(5,4))
plt.plot(rec, prec, label=f"AP={average_precision_score(y,oof_proba):.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("PR (OOF)"); plt.legend(loc="lower left")
out_pr = os.path.join(base_dir, "ps2_test_cv_PR.png")
plt.savefig(out_pr, dpi=150, bbox_inches="tight"); plt.close()
print(f"[INFO] 已保存：{out_pr}")

plt.figure(figsize=(6,4))
plt.hist(oof_proba[y==0], bins=20, alpha=0.6, label="neg")
plt.hist(oof_proba[y==1], bins=20, alpha=0.6, label="pos")
plt.axvline(best_thr, color="r", linestyle="--", label=f"thr={best_thr:.3f}")
plt.xlabel("Predicted probability"); plt.ylabel("Count"); plt.title("OOF probability histogram")
plt.legend()
out_hist = os.path.join(base_dir, "ps2_test_cv_proba_hist.png")
plt.savefig(out_hist, dpi=150, bbox_inches="tight"); plt.close()
print(f"[INFO] 已保存：{out_hist}")

# 误判/不确定清单
rep = pd.DataFrame({
    "SeqID": df["SeqID"] if "SeqID" in df.columns else np.arange(len(y)),
    "Target": y, "proba": oof_proba, "pred": oof_pred
})
rep["margin"] = np.abs(oof_proba - best_thr)
mis = rep[rep["pred"] != rep["Target"]].copy().sort_values("margin", ascending=False).head(30)
unc = rep.sort_values("margin", ascending=True).head(30)
mis.to_csv(os.path.join(base_dir, "ps2_test_cv_misclassified_top.csv"), index=False)
unc.to_csv(os.path.join(base_dir, "ps2_test_cv_uncertain_top.csv"), index=False)
print("[INFO] 已保存：misclassified_top.csv / uncertain_top.csv")

# 解释性：位置/二联体聚合和频率热图（假设 40×16）
DI = ["AA","AC","AG","AU","CA","CC","CG","CU","GA","GC","GG","GU","UA","UC","UG","UU"]
POS_COUNT, PER_POS = 40, 16

# 构造列的 pos/di 映射：若列名已为 pos_i_DI 则解析，否则按顺序映射
def parse_pos_di(cname, idx):
    if isinstance(cname, str) and cname.startswith("pos_"):
        try:
            parts = cname.split("_"); i = int(parts[1]); di = parts[2]
            if di not in DI: i = idx//16; di = DI[idx%16]
        except Exception:
            i = idx//16; di = DI[idx%16]
    else:
        i = idx//16; di = DI[idx%16]
    return i, di

map_rows = []
for k,c in enumerate(cols):
    i,di = parse_pos_di(c,k)
    map_rows.append((c,k,i,di))
map_df = pd.DataFrame(map_rows, columns=["feature","feat_idx","pos","di"])

imp_df = pd.DataFrame({"feature": cols, "importance": imp}).merge(map_df, on="feature", how="left")
pos_imp = imp_df.groupby("pos")["importance"].sum().sort_values(ascending=False)
di_imp  = imp_df.groupby("di")["importance"].sum().reindex(DI).fillna(0.0)

pos_imp.to_csv(os.path.join(base_dir, "ps2_test_position_importance.csv"))
di_imp.to_csv(os.path.join(base_dir, "ps2_test_dinucleotide_importance.csv"))

plt.figure(figsize=(10,3))
plt.bar(pos_imp.index, pos_imp.values)
plt.xlabel("Position (0..39)"); plt.ylabel("Aggregated importance"); plt.title("Position-wise importance")
plt.savefig(os.path.join(base_dir, "ps2_test_position_importance.png"), dpi=150, bbox_inches="tight"); plt.close()

plt.figure(figsize=(8,3))
plt.bar(range(len(DI)), di_imp.values, tick_label=DI)
plt.xlabel("Dinucleotide"); plt.ylabel("Aggregated importance"); plt.title("Dinucleotide-wise importance")
plt.savefig(os.path.join(base_dir, "ps2_test_dinucleotide_importance.png"), dpi=150, bbox_inches="tight"); plt.close()

# 频率热图
def freq_matrix(X_mat):
    return X_mat.mean(axis=0).reshape(POS_COUNT, PER_POS)

X_np = X.values.astype(float)
f_all = freq_matrix(X_np)
f_pos = freq_matrix(X_np[y==1]) if (y==1).any() else np.zeros_like(f_all)
f_neg = freq_matrix(X_np[y==0]) if (y==0).any() else np.zeros_like(f_all)
f_diff = f_pos - f_neg

def plot_heat(mat, title, out_path, vmin=None, vmax=None, cmap="viridis"):
    plt.figure(figsize=(10,5))
    im = plt.imshow(mat.T, aspect="auto", origin="lower", cmap=cmap, vmin=vmin, vmax=vmax)
    plt.colorbar(im, fraction=0.02)
    plt.yticks(range(len(DI)), DI)
    plt.xlabel("Position (0..39)"); plt.ylabel("Dinucleotide")
    plt.title(title); plt.tight_layout()
    plt.savefig(out_path, dpi=150); plt.close()

plot_heat(f_pos, "Frequency (Positive)", os.path.join(base_dir, "ps2_test_freq_pos.png"))
plot_heat(f_neg, "Frequency (Negative)", os.path.join(base_dir, "ps2_test_freq_neg.png"))
mx = np.max(np.abs(f_diff))
plot_heat(f_diff, "Frequency difference (pos - neg)", os.path.join(base_dir, "ps2_test_freq_diff.png"),
          vmin=-mx, vmax=mx, cmap="bwr")

# 偏好Top-20
diff_flat = []
for i in range(POS_COUNT):
    for j, di in enumerate(DI):
        diff_flat.append((i, di, f_diff[i, j]))
diff_df = pd.DataFrame(diff_flat, columns=["pos","di","pos_minus_neg_freq"])
top_pos_pref = diff_df.sort_values("pos_minus_neg_freq", ascending=False).head(20)
top_neg_pref = diff_df.sort_values("pos_minus_neg_freq", ascending=True).head(20)
pd.concat([top_pos_pref.assign(direction="pos>neg"),
           top_neg_pref.assign(direction="neg>pos")]).to_csv(
    os.path.join(base_dir, "ps2_test_posneg_pref_top20.csv"), index=False
)
print("[INFO] 评估与可视化完成。")
```

四、在 Colab 的推荐执行顺序
- 挂载 Drive
  - from google.colab import drive; drive.mount('/content/drive')
- 确认目录并创建 scripts 目录
  - import os; os.makedirs("/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main/scripts", exist_ok=True)
- 写入并运行生成特征脚本
  - 使用 Colab 单元：
    - %%writefile "/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main/scripts/make_test_from_fasta_ps2.py"
    - 粘贴上面的脚本内容
  - 运行
    - %run "/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main/scripts/make_test_from_fasta_ps2.py"
  - 期望输出：
    - 原始 PS2 特征形状：(122, 682) 或 (122, 642) 取决于列名文件；其中 680/640 为特征列，另含 SeqID 与 Target
    - 每行 1 的个数约为 40
- 写入并运行评估脚本
  - 同样用 %%writefile 写入 scripts/eval_ps2_cv.py
  - 运行
    - %run "/content/drive/MyDrive/phage virion proteins/DHUpredET-DHU-prediction-main/scripts/eval_ps2_cv.py"
  - 期望指标（与你贴出一致）：
    - ROC AUC ≈ 0.898
    - PR AUC ≈ 0.917
    - Accuracy ≈ 0.828
    - 混淆矩阵大致 [[50, 11], [10, 51]]
  - 生成的文件（都在 base_dir 下）：
    - ps2_test.csv（测试特征）
    - ps2_test_cv_pred.csv（OOF 概率与标签）
    - ps2_test_feature_importance_top30.csv（重要性 Top-30）
    - ps2_test_cv_ROC.png、ps2_test_cv_PR.png、ps2_test_cv_proba_hist.png（图）
    - ps2_test_position_importance.csv/.png（位置重要性）
    - ps2_test_dinucleotide_importance.csv/.png（二联体重要性）
    - ps2_test_freq_pos.png、ps2_test_freq_neg.png、ps2_test_freq_diff.png（频率热图）
    - ps2_test_cv_misclassified_top.csv（高置信误判）
    - ps2_test_cv_uncertain_top.csv（最不确定样本）
    - ps2_test_posneg_pref_top20.csv（偏好 Top-20）

五、如何解释这次的“最终结果”
- 模型排序能力强：ROC≈0.90，PR≈0.92，说明正负样本整体可分性良好。
- 在 F1 最优阈值附近，精度与召回均衡：Acc≈0.83，正负两类 F1 都≈0.83。
- 概率均值分离：正类均值≈0.659，负类≈0.379，分布分离明显。
- 重要性与频率热图可用于“位置×二联体”的可解释分析：
  - 哪些位置（pos_i）对区分更关键（位置重要性柱状图）
  - 哪些二联体类型（DI）全局更关键（二联体重要性）
  - 正负类在不同位置×二联体上的频率差异（红蓝热图）
- 错例与不确定样本清单能指导人工复核与数据清洗。

六、常见坑位与快速排查
- 全 0 或概率恒定：多半是用蛋白序列跑了 RNA 的 PS2 编码，或序列非 A/C/G/U。检查每行 1 的个数是否≈40、ACGU 覆盖率是否高。
- 列名/维度不一致：使用 reindex 对齐训练列，或直接用本流程（CV 不依赖训练列名）。
- 阈值不当导致全预测一类：用 OOF 概率做 F1 最优阈值寻优。

可选：需要从训练 FASTA 重建训练集
- 如需严谨的“独立测试”，可用同样编码脚本从 train_pos_modified.fasta / train_neg_modified.fasta 生成 ps2_train_from_fasta.csv，再按相同列名/顺序评估。需要的话我也可以提供训练集生成脚本。
